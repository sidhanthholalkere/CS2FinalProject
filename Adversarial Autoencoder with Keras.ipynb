{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Autoencoders using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "from keras.layers import Input, Dense, Activation, LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and hyperparameters\n",
    "rows, columns and channels dictate the properties of the input image\n",
    "latent_size is the size of the \"squished\" or encoded representation\n",
    "optimizer is just the optimizer\n",
    "epochs & batch_size are just training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sid/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# just some variables\n",
    "\n",
    "rows = 28\n",
    "cols = 28\n",
    "channels = 1\n",
    "img_shape = (rows, cols, channels)\n",
    "img_size = rows * cols * channels\n",
    "\n",
    "latent_size = 10\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "epochs = 5000\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "The encoder takes in a full sized image and compresses it to a smaller dimensionality, in this case it goes from img_size to latent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the encoder\n",
    "def create_encoder(latent_size, img_size):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(512, input_dim=img_size))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(Dense(512))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(Dense(latent_size))\n",
    "  \n",
    "  in_img = Input(shape=(img_size,))\n",
    "  encoded_repr = model(in_img)\n",
    "  \n",
    "  return Model(in_img, encoded_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "The decoder takes in a compressed version of the image and tries to recreate the original image. We are using sigmoid(from 0-1) because we normalize the images from 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the decoder\n",
    "def create_decoder(latent_size, img_size):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(512, input_dim=latent_size))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(Dense(512))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(Dense(img_size, activation='sigmoid'))\n",
    "  \n",
    "  encoded_repr = Input(shape=(latent_size,))\n",
    "  out_img = model(encoded_repr)\n",
    "  \n",
    "  return Model(encoded_repr, out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "This is the main part of *adversarial* autoencoders. The discriminator first learns how to tell whether an array belongs to a given distribution. The encoder then uses the discriminators output to try to make the encoded representation fit the given latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the discriminator\n",
    "def create_discriminator(latent_size):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(512, input_dim=latent_size))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(Dense(256))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "  encoded_repr = Input(shape=(latent_size,))\n",
    "  probability = model(encoded_repr)\n",
    "  \n",
    "  return Model(encoded_repr, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it together\n",
    "\n",
    "discriminator = create_discriminator(latent_size)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "encoder = create_encoder(latent_size, img_size)\n",
    "decoder = create_decoder(latent_size, img_size)\n",
    "\n",
    "img = Input(shape=(img_size,))\n",
    "\n",
    "encoded_repr = encoder(img)\n",
    "\n",
    "decoded = decoder(encoded_repr)\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "validity = discriminator(encoded_repr)\n",
    "\n",
    "adversarial = Model(img, [decoded, validity])\n",
    "adversarial.compile(loss=['mse', 'binary_crossentropy'], \n",
    "                          loss_weights=[0.999, 0.001], \n",
    "                          optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting and preprocessing the data\n",
    "We only need the training set because AAEs don't care about classification(some do, but not this one)\n",
    "Then we normalize the data to [0,1] and finally reshape the 28x28 images to 784x1 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and preprocess data\n",
    "\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(np.float64) / 255\n",
    "x_train = x_train.reshape(x_train.shape[0], img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sid/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sid/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.690654, acc: 34.77%] [G loss: 0.231352, mse: 0.230906]\n",
      "10 [D loss: 0.212701, acc: 97.66%] [G loss: 0.115636, mse: 0.110882]\n",
      "20 [D loss: 0.108238, acc: 99.61%] [G loss: 0.074324, mse: 0.069094]\n",
      "30 [D loss: 0.072832, acc: 100.00%] [G loss: 0.075493, mse: 0.070393]\n",
      "40 [D loss: 0.084835, acc: 98.44%] [G loss: 0.076633, mse: 0.070186]\n",
      "50 [D loss: 0.068241, acc: 97.66%] [G loss: 0.070625, mse: 0.063932]\n",
      "60 [D loss: 0.087013, acc: 96.48%] [G loss: 0.066440, mse: 0.060119]\n",
      "70 [D loss: 0.300746, acc: 86.72%] [G loss: 0.070006, mse: 0.061908]\n",
      "80 [D loss: 0.823973, acc: 62.50%] [G loss: 0.061947, mse: 0.056382]\n",
      "90 [D loss: 2.009838, acc: 32.42%] [G loss: 0.065888, mse: 0.059843]\n",
      "100 [D loss: 0.849680, acc: 57.81%] [G loss: 0.065149, mse: 0.059014]\n",
      "110 [D loss: 0.651141, acc: 65.23%] [G loss: 0.056076, mse: 0.051943]\n",
      "120 [D loss: 0.549159, acc: 69.14%] [G loss: 0.052877, mse: 0.048291]\n",
      "130 [D loss: 0.466202, acc: 69.14%] [G loss: 0.049886, mse: 0.046258]\n",
      "140 [D loss: 0.412697, acc: 76.95%] [G loss: 0.049334, mse: 0.046008]\n",
      "150 [D loss: 0.381867, acc: 81.64%] [G loss: 0.043010, mse: 0.040119]\n",
      "160 [D loss: 0.454159, acc: 78.52%] [G loss: 0.044661, mse: 0.042376]\n",
      "170 [D loss: 0.413662, acc: 78.52%] [G loss: 0.045886, mse: 0.043201]\n",
      "180 [D loss: 0.394679, acc: 80.86%] [G loss: 0.045121, mse: 0.042288]\n",
      "190 [D loss: 0.380440, acc: 83.98%] [G loss: 0.044026, mse: 0.041446]\n",
      "200 [D loss: 0.405343, acc: 80.86%] [G loss: 0.044662, mse: 0.042122]\n",
      "210 [D loss: 0.333817, acc: 86.72%] [G loss: 0.045090, mse: 0.042005]\n",
      "220 [D loss: 0.360488, acc: 85.55%] [G loss: 0.038822, mse: 0.036211]\n",
      "230 [D loss: 0.294403, acc: 94.14%] [G loss: 0.039070, mse: 0.036538]\n",
      "240 [D loss: 0.275430, acc: 94.53%] [G loss: 0.040548, mse: 0.038111]\n",
      "250 [D loss: 0.271820, acc: 93.75%] [G loss: 0.040019, mse: 0.037491]\n",
      "260 [D loss: 0.265299, acc: 95.70%] [G loss: 0.038762, mse: 0.036184]\n",
      "270 [D loss: 0.257151, acc: 96.88%] [G loss: 0.039326, mse: 0.036789]\n",
      "280 [D loss: 0.266679, acc: 95.31%] [G loss: 0.037922, mse: 0.035624]\n",
      "290 [D loss: 0.216375, acc: 97.66%] [G loss: 0.037370, mse: 0.034543]\n",
      "300 [D loss: 0.231180, acc: 96.88%] [G loss: 0.037101, mse: 0.034661]\n",
      "310 [D loss: 0.247560, acc: 93.75%] [G loss: 0.035663, mse: 0.033284]\n",
      "320 [D loss: 0.265179, acc: 93.36%] [G loss: 0.037810, mse: 0.035396]\n",
      "330 [D loss: 0.216416, acc: 97.27%] [G loss: 0.035336, mse: 0.032821]\n",
      "340 [D loss: 0.270178, acc: 92.97%] [G loss: 0.035224, mse: 0.033012]\n",
      "350 [D loss: 0.226198, acc: 96.88%] [G loss: 0.034485, mse: 0.032108]\n",
      "360 [D loss: 0.241594, acc: 92.97%] [G loss: 0.034622, mse: 0.032118]\n",
      "370 [D loss: 0.249355, acc: 93.36%] [G loss: 0.033051, mse: 0.030589]\n",
      "380 [D loss: 0.260283, acc: 92.58%] [G loss: 0.035271, mse: 0.032766]\n",
      "390 [D loss: 0.266681, acc: 91.41%] [G loss: 0.034538, mse: 0.032321]\n",
      "400 [D loss: 0.296165, acc: 91.41%] [G loss: 0.032781, mse: 0.030781]\n",
      "410 [D loss: 0.292580, acc: 90.62%] [G loss: 0.032683, mse: 0.030494]\n",
      "420 [D loss: 0.306739, acc: 89.84%] [G loss: 0.031715, mse: 0.029558]\n",
      "430 [D loss: 0.335181, acc: 88.67%] [G loss: 0.032652, mse: 0.030649]\n",
      "440 [D loss: 0.346882, acc: 84.77%] [G loss: 0.031063, mse: 0.029040]\n",
      "450 [D loss: 0.338944, acc: 87.89%] [G loss: 0.029627, mse: 0.027752]\n",
      "460 [D loss: 0.318970, acc: 88.28%] [G loss: 0.031231, mse: 0.029230]\n",
      "470 [D loss: 0.365332, acc: 86.33%] [G loss: 0.032694, mse: 0.030817]\n",
      "480 [D loss: 0.416047, acc: 80.47%] [G loss: 0.031825, mse: 0.030165]\n",
      "490 [D loss: 0.389349, acc: 84.38%] [G loss: 0.030708, mse: 0.028995]\n",
      "500 [D loss: 0.432108, acc: 83.20%] [G loss: 0.030986, mse: 0.029280]\n",
      "510 [D loss: 0.413478, acc: 82.42%] [G loss: 0.030565, mse: 0.028973]\n",
      "520 [D loss: 0.483753, acc: 76.95%] [G loss: 0.031420, mse: 0.029950]\n",
      "530 [D loss: 0.438880, acc: 79.69%] [G loss: 0.028651, mse: 0.027080]\n",
      "540 [D loss: 0.418902, acc: 80.86%] [G loss: 0.028713, mse: 0.027177]\n",
      "550 [D loss: 0.507967, acc: 76.56%] [G loss: 0.028552, mse: 0.027166]\n",
      "560 [D loss: 0.496639, acc: 79.30%] [G loss: 0.030014, mse: 0.028634]\n",
      "570 [D loss: 0.535916, acc: 73.05%] [G loss: 0.030609, mse: 0.029303]\n",
      "580 [D loss: 0.519311, acc: 75.78%] [G loss: 0.028873, mse: 0.027466]\n",
      "590 [D loss: 0.489820, acc: 75.00%] [G loss: 0.030624, mse: 0.029392]\n",
      "600 [D loss: 0.553643, acc: 69.53%] [G loss: 0.028113, mse: 0.026859]\n",
      "610 [D loss: 0.609498, acc: 68.36%] [G loss: 0.028597, mse: 0.027399]\n",
      "620 [D loss: 0.530226, acc: 72.66%] [G loss: 0.026892, mse: 0.025656]\n",
      "630 [D loss: 0.576590, acc: 72.27%] [G loss: 0.030403, mse: 0.029207]\n",
      "640 [D loss: 0.575643, acc: 71.48%] [G loss: 0.027174, mse: 0.025962]\n",
      "650 [D loss: 0.615667, acc: 70.31%] [G loss: 0.027400, mse: 0.026270]\n",
      "660 [D loss: 0.643022, acc: 63.28%] [G loss: 0.027649, mse: 0.026601]\n",
      "670 [D loss: 0.611981, acc: 69.53%] [G loss: 0.027572, mse: 0.026475]\n",
      "680 [D loss: 0.606422, acc: 69.14%] [G loss: 0.025587, mse: 0.024515]\n",
      "690 [D loss: 0.642326, acc: 70.70%] [G loss: 0.027350, mse: 0.026287]\n",
      "700 [D loss: 0.649958, acc: 65.62%] [G loss: 0.027453, mse: 0.026462]\n",
      "710 [D loss: 0.679561, acc: 63.67%] [G loss: 0.029522, mse: 0.028607]\n",
      "720 [D loss: 0.596365, acc: 70.31%] [G loss: 0.025571, mse: 0.024551]\n",
      "730 [D loss: 0.637856, acc: 66.41%] [G loss: 0.025857, mse: 0.024838]\n",
      "740 [D loss: 0.638626, acc: 66.41%] [G loss: 0.026947, mse: 0.025991]\n",
      "750 [D loss: 0.649265, acc: 65.23%] [G loss: 0.026172, mse: 0.025238]\n",
      "760 [D loss: 0.689471, acc: 60.16%] [G loss: 0.029093, mse: 0.028219]\n",
      "770 [D loss: 0.630547, acc: 64.45%] [G loss: 0.027256, mse: 0.026262]\n",
      "780 [D loss: 0.667999, acc: 64.45%] [G loss: 0.026753, mse: 0.025804]\n",
      "790 [D loss: 0.659900, acc: 63.67%] [G loss: 0.025948, mse: 0.025101]\n",
      "800 [D loss: 0.678779, acc: 63.28%] [G loss: 0.025229, mse: 0.024320]\n",
      "810 [D loss: 0.650161, acc: 67.58%] [G loss: 0.024443, mse: 0.023511]\n",
      "820 [D loss: 0.667093, acc: 62.89%] [G loss: 0.028336, mse: 0.027451]\n",
      "830 [D loss: 0.697871, acc: 57.42%] [G loss: 0.025715, mse: 0.024790]\n",
      "840 [D loss: 0.650447, acc: 64.84%] [G loss: 0.024298, mse: 0.023370]\n",
      "850 [D loss: 0.685315, acc: 58.98%] [G loss: 0.027614, mse: 0.026718]\n",
      "860 [D loss: 0.689194, acc: 63.67%] [G loss: 0.026141, mse: 0.025316]\n",
      "870 [D loss: 0.731312, acc: 58.98%] [G loss: 0.027044, mse: 0.026169]\n",
      "880 [D loss: 0.678068, acc: 62.50%] [G loss: 0.025582, mse: 0.024724]\n",
      "890 [D loss: 0.689198, acc: 60.94%] [G loss: 0.024339, mse: 0.023470]\n",
      "900 [D loss: 0.707012, acc: 60.55%] [G loss: 0.025728, mse: 0.024876]\n",
      "910 [D loss: 0.671266, acc: 62.11%] [G loss: 0.024307, mse: 0.023425]\n",
      "920 [D loss: 0.651953, acc: 66.02%] [G loss: 0.025492, mse: 0.024634]\n",
      "930 [D loss: 0.672458, acc: 59.38%] [G loss: 0.026691, mse: 0.025832]\n",
      "940 [D loss: 0.703439, acc: 59.77%] [G loss: 0.024607, mse: 0.023763]\n",
      "950 [D loss: 0.672718, acc: 60.94%] [G loss: 0.026683, mse: 0.025791]\n",
      "960 [D loss: 0.604897, acc: 71.48%] [G loss: 0.024229, mse: 0.023324]\n",
      "970 [D loss: 0.653662, acc: 64.45%] [G loss: 0.026299, mse: 0.025410]\n",
      "980 [D loss: 0.692433, acc: 55.86%] [G loss: 0.025212, mse: 0.024409]\n",
      "990 [D loss: 0.662444, acc: 60.94%] [G loss: 0.024275, mse: 0.023453]\n",
      "1000 [D loss: 0.693962, acc: 59.38%] [G loss: 0.024603, mse: 0.023780]\n",
      "1010 [D loss: 0.637143, acc: 65.23%] [G loss: 0.026190, mse: 0.025273]\n",
      "1020 [D loss: 0.641349, acc: 64.45%] [G loss: 0.024924, mse: 0.024070]\n",
      "1030 [D loss: 0.634804, acc: 61.72%] [G loss: 0.023982, mse: 0.023108]\n",
      "1040 [D loss: 0.658563, acc: 64.06%] [G loss: 0.023367, mse: 0.022486]\n",
      "1050 [D loss: 0.629044, acc: 65.62%] [G loss: 0.024654, mse: 0.023707]\n",
      "1060 [D loss: 0.664396, acc: 60.94%] [G loss: 0.023703, mse: 0.022908]\n",
      "1070 [D loss: 0.665192, acc: 63.28%] [G loss: 0.023770, mse: 0.022886]\n",
      "1080 [D loss: 0.656108, acc: 65.23%] [G loss: 0.023948, mse: 0.023068]\n",
      "1090 [D loss: 0.681040, acc: 59.77%] [G loss: 0.022712, mse: 0.021874]\n",
      "1100 [D loss: 0.682647, acc: 60.94%] [G loss: 0.025017, mse: 0.024208]\n",
      "1110 [D loss: 0.675683, acc: 59.38%] [G loss: 0.023745, mse: 0.022904]\n",
      "1120 [D loss: 0.684785, acc: 62.11%] [G loss: 0.023704, mse: 0.022883]\n",
      "1130 [D loss: 0.693704, acc: 58.98%] [G loss: 0.024930, mse: 0.024159]\n",
      "1140 [D loss: 0.639317, acc: 66.02%] [G loss: 0.022086, mse: 0.021259]\n",
      "1150 [D loss: 0.646497, acc: 65.62%] [G loss: 0.023817, mse: 0.022975]\n",
      "1160 [D loss: 0.658023, acc: 64.45%] [G loss: 0.024304, mse: 0.023464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170 [D loss: 0.684275, acc: 55.86%] [G loss: 0.024441, mse: 0.023619]\n",
      "1180 [D loss: 0.644888, acc: 62.50%] [G loss: 0.024425, mse: 0.023591]\n",
      "1190 [D loss: 0.653634, acc: 62.50%] [G loss: 0.023553, mse: 0.022728]\n",
      "1200 [D loss: 0.655416, acc: 61.33%] [G loss: 0.023969, mse: 0.023133]\n",
      "1210 [D loss: 0.643297, acc: 64.45%] [G loss: 0.024343, mse: 0.023477]\n",
      "1220 [D loss: 0.661585, acc: 62.50%] [G loss: 0.022657, mse: 0.021802]\n",
      "1230 [D loss: 0.641748, acc: 63.67%] [G loss: 0.023387, mse: 0.022502]\n",
      "1240 [D loss: 0.654229, acc: 60.94%] [G loss: 0.024241, mse: 0.023384]\n",
      "1250 [D loss: 0.676565, acc: 60.94%] [G loss: 0.025082, mse: 0.024261]\n",
      "1260 [D loss: 0.633439, acc: 61.33%] [G loss: 0.023918, mse: 0.023026]\n",
      "1270 [D loss: 0.606891, acc: 68.75%] [G loss: 0.022780, mse: 0.021877]\n",
      "1280 [D loss: 0.666462, acc: 60.16%] [G loss: 0.021597, mse: 0.020750]\n",
      "1290 [D loss: 0.656757, acc: 59.77%] [G loss: 0.023214, mse: 0.022384]\n",
      "1300 [D loss: 0.589286, acc: 73.44%] [G loss: 0.020907, mse: 0.020000]\n",
      "1310 [D loss: 0.654947, acc: 63.28%] [G loss: 0.023500, mse: 0.022684]\n",
      "1320 [D loss: 0.668536, acc: 61.72%] [G loss: 0.026660, mse: 0.025885]\n",
      "1330 [D loss: 0.646766, acc: 60.55%] [G loss: 0.024030, mse: 0.023245]\n",
      "1340 [D loss: 0.685172, acc: 62.11%] [G loss: 0.024324, mse: 0.023547]\n",
      "1350 [D loss: 0.638171, acc: 63.28%] [G loss: 0.023051, mse: 0.022229]\n",
      "1360 [D loss: 0.659884, acc: 64.45%] [G loss: 0.022590, mse: 0.021765]\n",
      "1370 [D loss: 0.645558, acc: 62.89%] [G loss: 0.022994, mse: 0.022154]\n",
      "1380 [D loss: 0.654030, acc: 61.72%] [G loss: 0.021901, mse: 0.021080]\n",
      "1390 [D loss: 0.617294, acc: 65.23%] [G loss: 0.020952, mse: 0.020121]\n",
      "1400 [D loss: 0.614693, acc: 66.80%] [G loss: 0.022412, mse: 0.021558]\n",
      "1410 [D loss: 0.646733, acc: 66.02%] [G loss: 0.021512, mse: 0.020652]\n",
      "1420 [D loss: 0.705536, acc: 57.03%] [G loss: 0.024354, mse: 0.023565]\n",
      "1430 [D loss: 0.612444, acc: 70.31%] [G loss: 0.022366, mse: 0.021476]\n",
      "1440 [D loss: 0.660483, acc: 60.16%] [G loss: 0.023254, mse: 0.022431]\n",
      "1450 [D loss: 0.634863, acc: 66.41%] [G loss: 0.022850, mse: 0.022040]\n",
      "1460 [D loss: 0.638372, acc: 65.23%] [G loss: 0.023486, mse: 0.022581]\n",
      "1470 [D loss: 0.645804, acc: 59.38%] [G loss: 0.022392, mse: 0.021580]\n",
      "1480 [D loss: 0.659170, acc: 61.72%] [G loss: 0.025477, mse: 0.024697]\n",
      "1490 [D loss: 0.653210, acc: 64.84%] [G loss: 0.023625, mse: 0.022817]\n",
      "1500 [D loss: 0.675582, acc: 63.28%] [G loss: 0.022105, mse: 0.021283]\n",
      "1510 [D loss: 0.661631, acc: 59.77%] [G loss: 0.023156, mse: 0.022378]\n",
      "1520 [D loss: 0.643650, acc: 64.84%] [G loss: 0.022821, mse: 0.021995]\n",
      "1530 [D loss: 0.584772, acc: 72.66%] [G loss: 0.022954, mse: 0.022098]\n",
      "1540 [D loss: 0.649460, acc: 65.23%] [G loss: 0.022576, mse: 0.021737]\n",
      "1550 [D loss: 0.653270, acc: 64.06%] [G loss: 0.022749, mse: 0.021938]\n",
      "1560 [D loss: 0.645886, acc: 61.72%] [G loss: 0.022881, mse: 0.021993]\n",
      "1570 [D loss: 0.644612, acc: 64.84%] [G loss: 0.020946, mse: 0.020098]\n",
      "1580 [D loss: 0.637916, acc: 64.45%] [G loss: 0.022524, mse: 0.021668]\n",
      "1590 [D loss: 0.605720, acc: 66.41%] [G loss: 0.021014, mse: 0.020134]\n",
      "1600 [D loss: 0.649598, acc: 64.45%] [G loss: 0.020893, mse: 0.020052]\n",
      "1610 [D loss: 0.639853, acc: 64.06%] [G loss: 0.021604, mse: 0.020749]\n",
      "1620 [D loss: 0.667736, acc: 57.81%] [G loss: 0.022669, mse: 0.021864]\n",
      "1630 [D loss: 0.639890, acc: 66.02%] [G loss: 0.020539, mse: 0.019707]\n",
      "1640 [D loss: 0.622161, acc: 65.23%] [G loss: 0.021597, mse: 0.020701]\n",
      "1650 [D loss: 0.625344, acc: 67.19%] [G loss: 0.022901, mse: 0.022055]\n",
      "1660 [D loss: 0.625853, acc: 65.62%] [G loss: 0.020805, mse: 0.019997]\n",
      "1670 [D loss: 0.652056, acc: 62.11%] [G loss: 0.023065, mse: 0.022267]\n",
      "1680 [D loss: 0.657029, acc: 63.28%] [G loss: 0.021877, mse: 0.021066]\n",
      "1690 [D loss: 0.632064, acc: 64.84%] [G loss: 0.022127, mse: 0.021264]\n",
      "1700 [D loss: 0.634740, acc: 66.41%] [G loss: 0.020513, mse: 0.019686]\n",
      "1710 [D loss: 0.598187, acc: 70.70%] [G loss: 0.020591, mse: 0.019722]\n",
      "1720 [D loss: 0.617682, acc: 66.41%] [G loss: 0.020996, mse: 0.020140]\n",
      "1730 [D loss: 0.602911, acc: 69.92%] [G loss: 0.021640, mse: 0.020748]\n",
      "1740 [D loss: 0.625031, acc: 66.41%] [G loss: 0.021334, mse: 0.020504]\n",
      "1750 [D loss: 0.640454, acc: 63.67%] [G loss: 0.021736, mse: 0.020910]\n",
      "1760 [D loss: 0.638375, acc: 61.33%] [G loss: 0.022540, mse: 0.021767]\n",
      "1770 [D loss: 0.628665, acc: 65.62%] [G loss: 0.020632, mse: 0.019791]\n",
      "1780 [D loss: 0.645573, acc: 62.50%] [G loss: 0.019954, mse: 0.019131]\n",
      "1790 [D loss: 0.666986, acc: 61.72%] [G loss: 0.020631, mse: 0.019793]\n",
      "1800 [D loss: 0.611984, acc: 70.31%] [G loss: 0.020461, mse: 0.019625]\n",
      "1810 [D loss: 0.644265, acc: 62.50%] [G loss: 0.021169, mse: 0.020374]\n",
      "1820 [D loss: 0.672817, acc: 58.59%] [G loss: 0.021163, mse: 0.020354]\n",
      "1830 [D loss: 0.649427, acc: 62.89%] [G loss: 0.021421, mse: 0.020634]\n",
      "1840 [D loss: 0.616053, acc: 63.28%] [G loss: 0.019376, mse: 0.018524]\n",
      "1850 [D loss: 0.621974, acc: 62.89%] [G loss: 0.022153, mse: 0.021319]\n",
      "1860 [D loss: 0.590592, acc: 71.88%] [G loss: 0.019633, mse: 0.018741]\n",
      "1870 [D loss: 0.621017, acc: 67.97%] [G loss: 0.020915, mse: 0.020066]\n",
      "1880 [D loss: 0.631448, acc: 64.06%] [G loss: 0.022103, mse: 0.021272]\n",
      "1890 [D loss: 0.631499, acc: 67.19%] [G loss: 0.021567, mse: 0.020743]\n",
      "1900 [D loss: 0.631942, acc: 65.62%] [G loss: 0.023537, mse: 0.022732]\n",
      "1910 [D loss: 0.639812, acc: 64.06%] [G loss: 0.020687, mse: 0.019840]\n",
      "1920 [D loss: 0.607898, acc: 64.84%] [G loss: 0.019987, mse: 0.019093]\n",
      "1930 [D loss: 0.615515, acc: 64.84%] [G loss: 0.021364, mse: 0.020492]\n",
      "1940 [D loss: 0.611343, acc: 69.14%] [G loss: 0.020917, mse: 0.020044]\n",
      "1950 [D loss: 0.651630, acc: 62.50%] [G loss: 0.022780, mse: 0.021987]\n",
      "1960 [D loss: 0.586844, acc: 68.75%] [G loss: 0.022392, mse: 0.021498]\n",
      "1970 [D loss: 0.657406, acc: 63.28%] [G loss: 0.021498, mse: 0.020658]\n",
      "1980 [D loss: 0.638896, acc: 61.72%] [G loss: 0.021466, mse: 0.020655]\n",
      "1990 [D loss: 0.617863, acc: 66.80%] [G loss: 0.022838, mse: 0.022018]\n",
      "2000 [D loss: 0.626171, acc: 65.23%] [G loss: 0.020925, mse: 0.020088]\n",
      "2010 [D loss: 0.639026, acc: 62.89%] [G loss: 0.020975, mse: 0.020169]\n",
      "2020 [D loss: 0.627748, acc: 64.45%] [G loss: 0.022290, mse: 0.021446]\n",
      "2030 [D loss: 0.631375, acc: 68.36%] [G loss: 0.021966, mse: 0.021157]\n",
      "2040 [D loss: 0.617725, acc: 65.62%] [G loss: 0.020260, mse: 0.019428]\n",
      "2050 [D loss: 0.608833, acc: 67.58%] [G loss: 0.020337, mse: 0.019458]\n",
      "2060 [D loss: 0.648228, acc: 61.72%] [G loss: 0.022288, mse: 0.021517]\n",
      "2070 [D loss: 0.633790, acc: 62.50%] [G loss: 0.022045, mse: 0.021195]\n",
      "2080 [D loss: 0.646806, acc: 62.11%] [G loss: 0.020841, mse: 0.020003]\n",
      "2090 [D loss: 0.610853, acc: 69.14%] [G loss: 0.020190, mse: 0.019347]\n",
      "2100 [D loss: 0.634346, acc: 66.02%] [G loss: 0.020943, mse: 0.020142]\n",
      "2110 [D loss: 0.604777, acc: 66.80%] [G loss: 0.020017, mse: 0.019131]\n",
      "2120 [D loss: 0.615391, acc: 66.80%] [G loss: 0.019471, mse: 0.018626]\n",
      "2130 [D loss: 0.595398, acc: 72.27%] [G loss: 0.020029, mse: 0.019138]\n",
      "2140 [D loss: 0.641159, acc: 62.50%] [G loss: 0.021481, mse: 0.020653]\n",
      "2150 [D loss: 0.599318, acc: 69.14%] [G loss: 0.020074, mse: 0.019222]\n",
      "2160 [D loss: 0.638641, acc: 64.45%] [G loss: 0.021272, mse: 0.020492]\n",
      "2170 [D loss: 0.601593, acc: 69.14%] [G loss: 0.019795, mse: 0.018928]\n",
      "2180 [D loss: 0.611745, acc: 69.92%] [G loss: 0.021613, mse: 0.020757]\n",
      "2190 [D loss: 0.614787, acc: 66.41%] [G loss: 0.020498, mse: 0.019652]\n",
      "2200 [D loss: 0.637686, acc: 67.97%] [G loss: 0.019967, mse: 0.019125]\n",
      "2210 [D loss: 0.583471, acc: 73.05%] [G loss: 0.020718, mse: 0.019874]\n",
      "2220 [D loss: 0.594744, acc: 72.27%] [G loss: 0.021016, mse: 0.020158]\n",
      "2230 [D loss: 0.588680, acc: 72.27%] [G loss: 0.018946, mse: 0.018010]\n",
      "2240 [D loss: 0.577599, acc: 74.61%] [G loss: 0.019554, mse: 0.018618]\n",
      "2250 [D loss: 0.642215, acc: 64.06%] [G loss: 0.020779, mse: 0.019934]\n",
      "2260 [D loss: 0.604790, acc: 68.75%] [G loss: 0.020836, mse: 0.019981]\n",
      "2270 [D loss: 0.633255, acc: 68.36%] [G loss: 0.019773, mse: 0.018954]\n",
      "2280 [D loss: 0.614924, acc: 67.58%] [G loss: 0.019524, mse: 0.018691]\n",
      "2290 [D loss: 0.616818, acc: 67.97%] [G loss: 0.021131, mse: 0.020259]\n",
      "2300 [D loss: 0.631172, acc: 66.80%] [G loss: 0.021133, mse: 0.020327]\n",
      "2310 [D loss: 0.607175, acc: 65.23%] [G loss: 0.021902, mse: 0.021060]\n",
      "2320 [D loss: 0.611053, acc: 69.53%] [G loss: 0.019422, mse: 0.018574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330 [D loss: 0.608908, acc: 68.75%] [G loss: 0.022844, mse: 0.022013]\n",
      "2340 [D loss: 0.603668, acc: 69.53%] [G loss: 0.020977, mse: 0.020073]\n",
      "2350 [D loss: 0.570980, acc: 73.44%] [G loss: 0.019521, mse: 0.018607]\n",
      "2360 [D loss: 0.596574, acc: 70.31%] [G loss: 0.019979, mse: 0.019107]\n",
      "2370 [D loss: 0.601010, acc: 68.75%] [G loss: 0.020169, mse: 0.019281]\n",
      "2380 [D loss: 0.589112, acc: 73.05%] [G loss: 0.019498, mse: 0.018629]\n",
      "2390 [D loss: 0.580403, acc: 72.27%] [G loss: 0.018924, mse: 0.018008]\n",
      "2400 [D loss: 0.598034, acc: 68.36%] [G loss: 0.020223, mse: 0.019387]\n",
      "2410 [D loss: 0.596334, acc: 68.75%] [G loss: 0.020400, mse: 0.019551]\n",
      "2420 [D loss: 0.643203, acc: 64.84%] [G loss: 0.021247, mse: 0.020440]\n",
      "2430 [D loss: 0.610695, acc: 69.14%] [G loss: 0.019834, mse: 0.018992]\n",
      "2440 [D loss: 0.605267, acc: 69.92%] [G loss: 0.019622, mse: 0.018757]\n",
      "2450 [D loss: 0.581479, acc: 71.48%] [G loss: 0.020712, mse: 0.019804]\n",
      "2460 [D loss: 0.573392, acc: 73.83%] [G loss: 0.020947, mse: 0.020056]\n",
      "2470 [D loss: 0.583532, acc: 74.61%] [G loss: 0.019721, mse: 0.018818]\n",
      "2480 [D loss: 0.601386, acc: 69.92%] [G loss: 0.019490, mse: 0.018591]\n",
      "2490 [D loss: 0.606174, acc: 68.36%] [G loss: 0.022292, mse: 0.021402]\n",
      "2500 [D loss: 0.583138, acc: 69.14%] [G loss: 0.020057, mse: 0.019155]\n",
      "2510 [D loss: 0.593919, acc: 69.14%] [G loss: 0.018943, mse: 0.018049]\n",
      "2520 [D loss: 0.596374, acc: 71.48%] [G loss: 0.020228, mse: 0.019325]\n",
      "2530 [D loss: 0.584368, acc: 73.05%] [G loss: 0.019926, mse: 0.019024]\n",
      "2540 [D loss: 0.619446, acc: 66.80%] [G loss: 0.022411, mse: 0.021560]\n",
      "2550 [D loss: 0.574461, acc: 73.83%] [G loss: 0.019190, mse: 0.018282]\n",
      "2560 [D loss: 0.579329, acc: 73.83%] [G loss: 0.019083, mse: 0.018141]\n",
      "2570 [D loss: 0.577890, acc: 76.17%] [G loss: 0.020111, mse: 0.019193]\n",
      "2580 [D loss: 0.551343, acc: 75.39%] [G loss: 0.020144, mse: 0.019214]\n",
      "2590 [D loss: 0.608134, acc: 71.48%] [G loss: 0.020230, mse: 0.019356]\n",
      "2600 [D loss: 0.579781, acc: 71.88%] [G loss: 0.019803, mse: 0.018903]\n",
      "2610 [D loss: 0.579445, acc: 73.83%] [G loss: 0.020812, mse: 0.019919]\n",
      "2620 [D loss: 0.587551, acc: 73.05%] [G loss: 0.019401, mse: 0.018488]\n",
      "2630 [D loss: 0.582526, acc: 71.09%] [G loss: 0.019173, mse: 0.018285]\n",
      "2640 [D loss: 0.578824, acc: 70.31%] [G loss: 0.021952, mse: 0.021011]\n",
      "2650 [D loss: 0.576478, acc: 74.61%] [G loss: 0.020049, mse: 0.019136]\n",
      "2660 [D loss: 0.574274, acc: 75.00%] [G loss: 0.020547, mse: 0.019638]\n",
      "2670 [D loss: 0.596128, acc: 66.80%] [G loss: 0.020682, mse: 0.019793]\n",
      "2680 [D loss: 0.556976, acc: 74.22%] [G loss: 0.018702, mse: 0.017766]\n",
      "2690 [D loss: 0.565103, acc: 74.61%] [G loss: 0.021202, mse: 0.020263]\n",
      "2700 [D loss: 0.567046, acc: 74.61%] [G loss: 0.020568, mse: 0.019658]\n",
      "2710 [D loss: 0.579576, acc: 71.09%] [G loss: 0.017722, mse: 0.016827]\n",
      "2720 [D loss: 0.580946, acc: 71.48%] [G loss: 0.021358, mse: 0.020461]\n",
      "2730 [D loss: 0.570794, acc: 71.88%] [G loss: 0.019550, mse: 0.018616]\n",
      "2740 [D loss: 0.564346, acc: 75.78%] [G loss: 0.018750, mse: 0.017806]\n",
      "2750 [D loss: 0.582552, acc: 72.27%] [G loss: 0.019236, mse: 0.018296]\n",
      "2760 [D loss: 0.573009, acc: 69.14%] [G loss: 0.021344, mse: 0.020407]\n",
      "2770 [D loss: 0.591311, acc: 73.05%] [G loss: 0.020060, mse: 0.019190]\n",
      "2780 [D loss: 0.552291, acc: 74.22%] [G loss: 0.019786, mse: 0.018829]\n",
      "2790 [D loss: 0.542908, acc: 77.73%] [G loss: 0.018262, mse: 0.017291]\n",
      "2800 [D loss: 0.569192, acc: 70.70%] [G loss: 0.020339, mse: 0.019397]\n",
      "2810 [D loss: 0.587742, acc: 68.75%] [G loss: 0.019910, mse: 0.018989]\n",
      "2820 [D loss: 0.605020, acc: 69.53%] [G loss: 0.018817, mse: 0.017948]\n",
      "2830 [D loss: 0.572414, acc: 73.44%] [G loss: 0.019137, mse: 0.018162]\n",
      "2840 [D loss: 0.534214, acc: 78.52%] [G loss: 0.019433, mse: 0.018450]\n",
      "2850 [D loss: 0.553004, acc: 67.58%] [G loss: 0.020128, mse: 0.019159]\n",
      "2860 [D loss: 0.568967, acc: 71.88%] [G loss: 0.019835, mse: 0.018919]\n",
      "2870 [D loss: 0.594119, acc: 69.14%] [G loss: 0.019049, mse: 0.018156]\n",
      "2880 [D loss: 0.554135, acc: 77.73%] [G loss: 0.020140, mse: 0.019154]\n",
      "2890 [D loss: 0.569671, acc: 75.39%] [G loss: 0.020880, mse: 0.019944]\n",
      "2900 [D loss: 0.546997, acc: 76.17%] [G loss: 0.020667, mse: 0.019687]\n",
      "2910 [D loss: 0.576266, acc: 71.48%] [G loss: 0.020241, mse: 0.019288]\n",
      "2920 [D loss: 0.535606, acc: 75.39%] [G loss: 0.018984, mse: 0.017985]\n",
      "2930 [D loss: 0.560473, acc: 71.48%] [G loss: 0.018506, mse: 0.017553]\n",
      "2940 [D loss: 0.539464, acc: 78.12%] [G loss: 0.019416, mse: 0.018403]\n",
      "2950 [D loss: 0.544231, acc: 75.78%] [G loss: 0.020867, mse: 0.019895]\n",
      "2960 [D loss: 0.589344, acc: 71.09%] [G loss: 0.020277, mse: 0.019349]\n",
      "2970 [D loss: 0.539995, acc: 76.17%] [G loss: 0.020238, mse: 0.019202]\n",
      "2980 [D loss: 0.574913, acc: 71.88%] [G loss: 0.019278, mse: 0.018315]\n",
      "2990 [D loss: 0.589044, acc: 70.70%] [G loss: 0.020653, mse: 0.019724]\n",
      "3000 [D loss: 0.556797, acc: 74.61%] [G loss: 0.019397, mse: 0.018403]\n",
      "3010 [D loss: 0.549197, acc: 72.66%] [G loss: 0.018149, mse: 0.017134]\n",
      "3020 [D loss: 0.518489, acc: 78.91%] [G loss: 0.018563, mse: 0.017512]\n",
      "3030 [D loss: 0.540152, acc: 80.47%] [G loss: 0.019599, mse: 0.018605]\n",
      "3040 [D loss: 0.537377, acc: 78.52%] [G loss: 0.017124, mse: 0.016088]\n",
      "3050 [D loss: 0.547939, acc: 73.05%] [G loss: 0.019506, mse: 0.018522]\n",
      "3060 [D loss: 0.555745, acc: 72.27%] [G loss: 0.019932, mse: 0.018937]\n",
      "3070 [D loss: 0.570912, acc: 73.05%] [G loss: 0.020268, mse: 0.019331]\n",
      "3080 [D loss: 0.557241, acc: 75.78%] [G loss: 0.019351, mse: 0.018400]\n",
      "3090 [D loss: 0.550993, acc: 74.22%] [G loss: 0.019287, mse: 0.018278]\n",
      "3100 [D loss: 0.514323, acc: 81.64%] [G loss: 0.018629, mse: 0.017551]\n",
      "3110 [D loss: 0.541527, acc: 77.73%] [G loss: 0.019723, mse: 0.018695]\n",
      "3120 [D loss: 0.529558, acc: 78.52%] [G loss: 0.018246, mse: 0.017236]\n",
      "3130 [D loss: 0.553315, acc: 74.61%] [G loss: 0.021375, mse: 0.020413]\n",
      "3140 [D loss: 0.521187, acc: 79.69%] [G loss: 0.017111, mse: 0.016105]\n",
      "3150 [D loss: 0.544330, acc: 73.83%] [G loss: 0.019278, mse: 0.018231]\n",
      "3160 [D loss: 0.526988, acc: 80.47%] [G loss: 0.018808, mse: 0.017782]\n",
      "3170 [D loss: 0.540223, acc: 73.05%] [G loss: 0.019568, mse: 0.018528]\n",
      "3180 [D loss: 0.515026, acc: 78.91%] [G loss: 0.019712, mse: 0.018671]\n",
      "3190 [D loss: 0.549799, acc: 75.39%] [G loss: 0.019973, mse: 0.018981]\n",
      "3200 [D loss: 0.562029, acc: 72.66%] [G loss: 0.018587, mse: 0.017588]\n",
      "3210 [D loss: 0.530572, acc: 76.56%] [G loss: 0.018707, mse: 0.017588]\n",
      "3220 [D loss: 0.533065, acc: 75.00%] [G loss: 0.020096, mse: 0.019061]\n",
      "3230 [D loss: 0.568029, acc: 73.44%] [G loss: 0.018499, mse: 0.017483]\n",
      "3240 [D loss: 0.548883, acc: 71.88%] [G loss: 0.020574, mse: 0.019587]\n",
      "3250 [D loss: 0.518583, acc: 78.52%] [G loss: 0.018882, mse: 0.017812]\n",
      "3260 [D loss: 0.548436, acc: 71.88%] [G loss: 0.019662, mse: 0.018587]\n",
      "3270 [D loss: 0.555952, acc: 72.66%] [G loss: 0.018907, mse: 0.017866]\n",
      "3280 [D loss: 0.543026, acc: 76.17%] [G loss: 0.019092, mse: 0.017997]\n",
      "3290 [D loss: 0.528903, acc: 77.73%] [G loss: 0.019992, mse: 0.018930]\n",
      "3300 [D loss: 0.519179, acc: 76.95%] [G loss: 0.018132, mse: 0.017029]\n",
      "3310 [D loss: 0.580494, acc: 69.92%] [G loss: 0.019777, mse: 0.018792]\n",
      "3320 [D loss: 0.525673, acc: 73.83%] [G loss: 0.017102, mse: 0.016007]\n",
      "3330 [D loss: 0.555242, acc: 72.66%] [G loss: 0.018755, mse: 0.017670]\n",
      "3340 [D loss: 0.544926, acc: 76.95%] [G loss: 0.018889, mse: 0.017822]\n",
      "3350 [D loss: 0.556514, acc: 72.66%] [G loss: 0.017867, mse: 0.016829]\n",
      "3360 [D loss: 0.529239, acc: 71.09%] [G loss: 0.019960, mse: 0.018872]\n",
      "3370 [D loss: 0.523958, acc: 76.17%] [G loss: 0.019878, mse: 0.018781]\n",
      "3380 [D loss: 0.574982, acc: 70.70%] [G loss: 0.018811, mse: 0.017786]\n",
      "3390 [D loss: 0.543960, acc: 75.00%] [G loss: 0.018453, mse: 0.017384]\n",
      "3400 [D loss: 0.506332, acc: 78.12%] [G loss: 0.019030, mse: 0.017916]\n",
      "3410 [D loss: 0.475675, acc: 83.20%] [G loss: 0.018681, mse: 0.017526]\n",
      "3420 [D loss: 0.558612, acc: 71.88%] [G loss: 0.019104, mse: 0.018057]\n",
      "3430 [D loss: 0.519463, acc: 76.95%] [G loss: 0.018708, mse: 0.017567]\n",
      "3440 [D loss: 0.532973, acc: 72.27%] [G loss: 0.019402, mse: 0.018283]\n",
      "3450 [D loss: 0.494874, acc: 80.86%] [G loss: 0.020146, mse: 0.019024]\n",
      "3460 [D loss: 0.513942, acc: 75.39%] [G loss: 0.018758, mse: 0.017565]\n",
      "3470 [D loss: 0.528446, acc: 73.44%] [G loss: 0.018667, mse: 0.017591]\n",
      "3480 [D loss: 0.535510, acc: 74.22%] [G loss: 0.019157, mse: 0.018059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3490 [D loss: 0.516818, acc: 75.39%] [G loss: 0.020315, mse: 0.019295]\n",
      "3500 [D loss: 0.525777, acc: 76.95%] [G loss: 0.018792, mse: 0.017686]\n",
      "3510 [D loss: 0.508118, acc: 76.95%] [G loss: 0.019174, mse: 0.018062]\n",
      "3520 [D loss: 0.522576, acc: 78.12%] [G loss: 0.018441, mse: 0.017323]\n",
      "3530 [D loss: 0.512389, acc: 75.00%] [G loss: 0.018431, mse: 0.017286]\n",
      "3540 [D loss: 0.519514, acc: 77.34%] [G loss: 0.018227, mse: 0.017144]\n",
      "3550 [D loss: 0.510173, acc: 77.73%] [G loss: 0.017435, mse: 0.016270]\n",
      "3560 [D loss: 0.544854, acc: 73.83%] [G loss: 0.019286, mse: 0.018238]\n",
      "3570 [D loss: 0.543173, acc: 75.39%] [G loss: 0.017781, mse: 0.016664]\n",
      "3580 [D loss: 0.519036, acc: 74.61%] [G loss: 0.020251, mse: 0.019168]\n",
      "3590 [D loss: 0.494329, acc: 78.91%] [G loss: 0.018216, mse: 0.017040]\n",
      "3600 [D loss: 0.532226, acc: 75.00%] [G loss: 0.017852, mse: 0.016784]\n",
      "3610 [D loss: 0.482759, acc: 82.03%] [G loss: 0.018789, mse: 0.017642]\n",
      "3620 [D loss: 0.526595, acc: 75.78%] [G loss: 0.018996, mse: 0.017856]\n",
      "3630 [D loss: 0.525251, acc: 73.05%] [G loss: 0.017986, mse: 0.016801]\n",
      "3640 [D loss: 0.492130, acc: 80.86%] [G loss: 0.020358, mse: 0.019174]\n",
      "3650 [D loss: 0.501960, acc: 80.08%] [G loss: 0.018626, mse: 0.017468]\n",
      "3660 [D loss: 0.551437, acc: 73.05%] [G loss: 0.019214, mse: 0.018098]\n",
      "3670 [D loss: 0.497045, acc: 77.34%] [G loss: 0.018398, mse: 0.017248]\n",
      "3680 [D loss: 0.525658, acc: 75.39%] [G loss: 0.018926, mse: 0.017827]\n",
      "3690 [D loss: 0.522680, acc: 75.00%] [G loss: 0.018243, mse: 0.016993]\n",
      "3700 [D loss: 0.502612, acc: 78.52%] [G loss: 0.018163, mse: 0.017012]\n",
      "3710 [D loss: 0.516196, acc: 76.17%] [G loss: 0.016960, mse: 0.015782]\n",
      "3720 [D loss: 0.506354, acc: 76.56%] [G loss: 0.018305, mse: 0.017096]\n",
      "3730 [D loss: 0.543872, acc: 75.00%] [G loss: 0.018916, mse: 0.017812]\n",
      "3740 [D loss: 0.510434, acc: 75.78%] [G loss: 0.019323, mse: 0.018151]\n",
      "3750 [D loss: 0.535274, acc: 74.22%] [G loss: 0.018602, mse: 0.017592]\n",
      "3760 [D loss: 0.483704, acc: 80.47%] [G loss: 0.019751, mse: 0.018542]\n",
      "3770 [D loss: 0.542472, acc: 75.39%] [G loss: 0.019780, mse: 0.018673]\n",
      "3780 [D loss: 0.508243, acc: 77.34%] [G loss: 0.018487, mse: 0.017323]\n",
      "3790 [D loss: 0.493314, acc: 82.03%] [G loss: 0.018211, mse: 0.016979]\n",
      "3800 [D loss: 0.522925, acc: 73.83%] [G loss: 0.018996, mse: 0.017908]\n",
      "3810 [D loss: 0.509091, acc: 74.61%] [G loss: 0.019138, mse: 0.018056]\n",
      "3820 [D loss: 0.494020, acc: 80.08%] [G loss: 0.018687, mse: 0.017503]\n",
      "3830 [D loss: 0.526318, acc: 75.39%] [G loss: 0.019870, mse: 0.018813]\n",
      "3840 [D loss: 0.534855, acc: 75.00%] [G loss: 0.017832, mse: 0.016693]\n",
      "3850 [D loss: 0.515977, acc: 78.91%] [G loss: 0.018956, mse: 0.017803]\n",
      "3860 [D loss: 0.478053, acc: 81.64%] [G loss: 0.017576, mse: 0.016304]\n",
      "3870 [D loss: 0.511982, acc: 78.52%] [G loss: 0.020796, mse: 0.019636]\n",
      "3880 [D loss: 0.524739, acc: 76.17%] [G loss: 0.019905, mse: 0.018790]\n",
      "3890 [D loss: 0.511948, acc: 77.34%] [G loss: 0.018195, mse: 0.017009]\n",
      "3900 [D loss: 0.518068, acc: 80.08%] [G loss: 0.018446, mse: 0.017313]\n",
      "3910 [D loss: 0.540916, acc: 71.88%] [G loss: 0.017859, mse: 0.016730]\n",
      "3920 [D loss: 0.517639, acc: 75.00%] [G loss: 0.017957, mse: 0.016807]\n",
      "3930 [D loss: 0.520206, acc: 73.05%] [G loss: 0.017731, mse: 0.016578]\n",
      "3940 [D loss: 0.507339, acc: 75.78%] [G loss: 0.018832, mse: 0.017691]\n",
      "3950 [D loss: 0.515111, acc: 77.34%] [G loss: 0.018353, mse: 0.017283]\n",
      "3960 [D loss: 0.531327, acc: 71.88%] [G loss: 0.018938, mse: 0.017777]\n",
      "3970 [D loss: 0.526025, acc: 76.17%] [G loss: 0.017414, mse: 0.016247]\n",
      "3980 [D loss: 0.492414, acc: 77.34%] [G loss: 0.019080, mse: 0.017910]\n",
      "3990 [D loss: 0.506646, acc: 75.78%] [G loss: 0.017062, mse: 0.015887]\n",
      "4000 [D loss: 0.533143, acc: 75.00%] [G loss: 0.018034, mse: 0.016836]\n",
      "4010 [D loss: 0.545690, acc: 72.66%] [G loss: 0.021098, mse: 0.019988]\n",
      "4020 [D loss: 0.490509, acc: 78.12%] [G loss: 0.019245, mse: 0.018000]\n",
      "4030 [D loss: 0.525473, acc: 75.00%] [G loss: 0.018777, mse: 0.017619]\n",
      "4040 [D loss: 0.552816, acc: 71.88%] [G loss: 0.018601, mse: 0.017482]\n",
      "4050 [D loss: 0.511415, acc: 75.78%] [G loss: 0.019267, mse: 0.018117]\n",
      "4060 [D loss: 0.544659, acc: 73.05%] [G loss: 0.019330, mse: 0.018268]\n",
      "4070 [D loss: 0.489989, acc: 78.52%] [G loss: 0.017780, mse: 0.016611]\n",
      "4080 [D loss: 0.496204, acc: 77.34%] [G loss: 0.020348, mse: 0.019181]\n",
      "4090 [D loss: 0.538549, acc: 72.66%] [G loss: 0.017953, mse: 0.016860]\n",
      "4100 [D loss: 0.490737, acc: 78.52%] [G loss: 0.018893, mse: 0.017712]\n",
      "4110 [D loss: 0.503805, acc: 79.69%] [G loss: 0.018316, mse: 0.017077]\n",
      "4120 [D loss: 0.474213, acc: 78.91%] [G loss: 0.018919, mse: 0.017721]\n",
      "4130 [D loss: 0.485950, acc: 79.69%] [G loss: 0.018820, mse: 0.017594]\n",
      "4140 [D loss: 0.486787, acc: 78.12%] [G loss: 0.017863, mse: 0.016684]\n",
      "4150 [D loss: 0.493376, acc: 78.91%] [G loss: 0.018079, mse: 0.016889]\n",
      "4160 [D loss: 0.522210, acc: 76.17%] [G loss: 0.018468, mse: 0.017298]\n",
      "4170 [D loss: 0.494544, acc: 76.56%] [G loss: 0.018092, mse: 0.016959]\n",
      "4180 [D loss: 0.512394, acc: 73.44%] [G loss: 0.019232, mse: 0.018075]\n",
      "4190 [D loss: 0.499489, acc: 77.73%] [G loss: 0.018345, mse: 0.017141]\n",
      "4200 [D loss: 0.558362, acc: 72.27%] [G loss: 0.019762, mse: 0.018692]\n",
      "4210 [D loss: 0.536353, acc: 73.83%] [G loss: 0.019357, mse: 0.018155]\n",
      "4220 [D loss: 0.503246, acc: 76.56%] [G loss: 0.017898, mse: 0.016687]\n",
      "4230 [D loss: 0.493231, acc: 78.91%] [G loss: 0.017708, mse: 0.016495]\n",
      "4240 [D loss: 0.519847, acc: 75.39%] [G loss: 0.017078, mse: 0.015933]\n",
      "4250 [D loss: 0.506010, acc: 76.17%] [G loss: 0.017689, mse: 0.016461]\n",
      "4260 [D loss: 0.494626, acc: 80.08%] [G loss: 0.019562, mse: 0.018341]\n",
      "4270 [D loss: 0.477963, acc: 81.64%] [G loss: 0.018499, mse: 0.017261]\n",
      "4280 [D loss: 0.494496, acc: 78.12%] [G loss: 0.018866, mse: 0.017665]\n",
      "4290 [D loss: 0.547996, acc: 72.27%] [G loss: 0.018718, mse: 0.017535]\n",
      "4300 [D loss: 0.480679, acc: 76.17%] [G loss: 0.017534, mse: 0.016266]\n",
      "4310 [D loss: 0.526801, acc: 76.17%] [G loss: 0.017479, mse: 0.016280]\n",
      "4320 [D loss: 0.510217, acc: 76.56%] [G loss: 0.016219, mse: 0.015056]\n",
      "4330 [D loss: 0.524048, acc: 76.95%] [G loss: 0.019410, mse: 0.018235]\n",
      "4340 [D loss: 0.528892, acc: 75.78%] [G loss: 0.017541, mse: 0.016373]\n",
      "4350 [D loss: 0.522740, acc: 76.17%] [G loss: 0.017398, mse: 0.016220]\n",
      "4360 [D loss: 0.523396, acc: 74.22%] [G loss: 0.018699, mse: 0.017564]\n",
      "4370 [D loss: 0.497839, acc: 76.56%] [G loss: 0.017965, mse: 0.016660]\n",
      "4380 [D loss: 0.539750, acc: 74.61%] [G loss: 0.018115, mse: 0.016944]\n",
      "4390 [D loss: 0.504648, acc: 75.78%] [G loss: 0.017999, mse: 0.016777]\n",
      "4400 [D loss: 0.526537, acc: 72.27%] [G loss: 0.020102, mse: 0.018979]\n",
      "4410 [D loss: 0.496666, acc: 79.69%] [G loss: 0.019937, mse: 0.018817]\n",
      "4420 [D loss: 0.541607, acc: 71.88%] [G loss: 0.017753, mse: 0.016536]\n",
      "4430 [D loss: 0.538114, acc: 75.39%] [G loss: 0.018705, mse: 0.017589]\n",
      "4440 [D loss: 0.488645, acc: 76.95%] [G loss: 0.019234, mse: 0.018020]\n",
      "4450 [D loss: 0.483360, acc: 78.52%] [G loss: 0.016328, mse: 0.014983]\n",
      "4460 [D loss: 0.497997, acc: 77.34%] [G loss: 0.018094, mse: 0.016803]\n",
      "4470 [D loss: 0.513665, acc: 79.30%] [G loss: 0.018797, mse: 0.017664]\n",
      "4480 [D loss: 0.515620, acc: 75.78%] [G loss: 0.018395, mse: 0.017175]\n",
      "4490 [D loss: 0.514568, acc: 76.17%] [G loss: 0.018950, mse: 0.017791]\n",
      "4500 [D loss: 0.532675, acc: 75.00%] [G loss: 0.017740, mse: 0.016556]\n",
      "4510 [D loss: 0.524511, acc: 74.61%] [G loss: 0.017287, mse: 0.016118]\n",
      "4520 [D loss: 0.531172, acc: 73.83%] [G loss: 0.017339, mse: 0.016195]\n",
      "4530 [D loss: 0.552585, acc: 74.61%] [G loss: 0.017842, mse: 0.016630]\n",
      "4540 [D loss: 0.509239, acc: 80.08%] [G loss: 0.017665, mse: 0.016494]\n",
      "4550 [D loss: 0.497247, acc: 77.73%] [G loss: 0.016993, mse: 0.015775]\n",
      "4560 [D loss: 0.537561, acc: 74.61%] [G loss: 0.020766, mse: 0.019571]\n",
      "4570 [D loss: 0.529491, acc: 74.22%] [G loss: 0.018962, mse: 0.017733]\n",
      "4580 [D loss: 0.519885, acc: 75.39%] [G loss: 0.017717, mse: 0.016478]\n",
      "4590 [D loss: 0.484863, acc: 79.30%] [G loss: 0.018620, mse: 0.017387]\n",
      "4600 [D loss: 0.486599, acc: 79.30%] [G loss: 0.017041, mse: 0.015818]\n",
      "4610 [D loss: 0.497002, acc: 78.52%] [G loss: 0.016483, mse: 0.015269]\n",
      "4620 [D loss: 0.514325, acc: 74.22%] [G loss: 0.017795, mse: 0.016639]\n",
      "4630 [D loss: 0.472008, acc: 76.56%] [G loss: 0.018340, mse: 0.017114]\n",
      "4640 [D loss: 0.500762, acc: 75.78%] [G loss: 0.016999, mse: 0.015772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4650 [D loss: 0.529944, acc: 75.78%] [G loss: 0.018960, mse: 0.017817]\n",
      "4660 [D loss: 0.488874, acc: 78.91%] [G loss: 0.017565, mse: 0.016324]\n",
      "4670 [D loss: 0.492241, acc: 76.17%] [G loss: 0.017680, mse: 0.016400]\n",
      "4680 [D loss: 0.502978, acc: 76.95%] [G loss: 0.018049, mse: 0.016813]\n",
      "4690 [D loss: 0.528250, acc: 73.05%] [G loss: 0.018912, mse: 0.017731]\n",
      "4700 [D loss: 0.547764, acc: 73.83%] [G loss: 0.018679, mse: 0.017494]\n",
      "4710 [D loss: 0.511847, acc: 76.17%] [G loss: 0.017181, mse: 0.015986]\n",
      "4720 [D loss: 0.499838, acc: 78.12%] [G loss: 0.018252, mse: 0.017008]\n",
      "4730 [D loss: 0.561771, acc: 68.36%] [G loss: 0.019047, mse: 0.017871]\n",
      "4740 [D loss: 0.500712, acc: 74.61%] [G loss: 0.017457, mse: 0.016247]\n",
      "4750 [D loss: 0.546242, acc: 73.05%] [G loss: 0.018057, mse: 0.016856]\n",
      "4760 [D loss: 0.473482, acc: 80.08%] [G loss: 0.019090, mse: 0.017894]\n",
      "4770 [D loss: 0.529951, acc: 75.39%] [G loss: 0.016348, mse: 0.015164]\n",
      "4780 [D loss: 0.501831, acc: 74.61%] [G loss: 0.017877, mse: 0.016603]\n",
      "4790 [D loss: 0.524510, acc: 74.61%] [G loss: 0.017402, mse: 0.016218]\n",
      "4800 [D loss: 0.536900, acc: 72.66%] [G loss: 0.017928, mse: 0.016798]\n",
      "4810 [D loss: 0.517877, acc: 74.61%] [G loss: 0.018940, mse: 0.017766]\n",
      "4820 [D loss: 0.501758, acc: 75.78%] [G loss: 0.017486, mse: 0.016222]\n",
      "4830 [D loss: 0.523272, acc: 76.56%] [G loss: 0.017548, mse: 0.016256]\n",
      "4840 [D loss: 0.512646, acc: 74.22%] [G loss: 0.017844, mse: 0.016667]\n",
      "4850 [D loss: 0.530763, acc: 76.17%] [G loss: 0.017602, mse: 0.016393]\n",
      "4860 [D loss: 0.486859, acc: 79.30%] [G loss: 0.016523, mse: 0.015317]\n",
      "4870 [D loss: 0.502210, acc: 73.05%] [G loss: 0.017655, mse: 0.016437]\n",
      "4880 [D loss: 0.504827, acc: 76.17%] [G loss: 0.017336, mse: 0.016182]\n",
      "4890 [D loss: 0.535835, acc: 73.83%] [G loss: 0.016939, mse: 0.015684]\n",
      "4900 [D loss: 0.533859, acc: 74.22%] [G loss: 0.015130, mse: 0.013899]\n",
      "4910 [D loss: 0.506474, acc: 76.17%] [G loss: 0.017949, mse: 0.016759]\n",
      "4920 [D loss: 0.512599, acc: 77.73%] [G loss: 0.018026, mse: 0.016788]\n",
      "4930 [D loss: 0.496662, acc: 76.17%] [G loss: 0.016704, mse: 0.015437]\n",
      "4940 [D loss: 0.485648, acc: 80.08%] [G loss: 0.016466, mse: 0.015134]\n",
      "4950 [D loss: 0.523010, acc: 75.78%] [G loss: 0.018083, mse: 0.016882]\n",
      "4960 [D loss: 0.473056, acc: 80.08%] [G loss: 0.016824, mse: 0.015562]\n",
      "4970 [D loss: 0.488874, acc: 80.08%] [G loss: 0.016585, mse: 0.015310]\n",
      "4980 [D loss: 0.490712, acc: 77.73%] [G loss: 0.017493, mse: 0.016269]\n",
      "4990 [D loss: 0.573487, acc: 68.36%] [G loss: 0.019715, mse: 0.018561]\n"
     ]
    }
   ],
   "source": [
    "# create real and fake \"answers\"\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  # Select a random batch of images\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    imgs = x_train[idx]\n",
    "\n",
    "    latent_fake = encoder.predict(imgs)\n",
    "    \n",
    "    # Here we generate the \"TRUE\" samples\n",
    "    latent_real = np.random.normal(size=(batch_size, latent_size))\n",
    "                      \n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(latent_real, valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(latent_fake, fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    # Train the generator\n",
    "    g_loss = adversarial.train_on_batch(imgs, [imgs, valid])\n",
    "\n",
    "    # Plot the progress (every 10th epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
